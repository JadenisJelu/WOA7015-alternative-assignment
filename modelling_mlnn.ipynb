{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Big6SRB8m9sL"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAYS_KF2xw6u",
        "outputId": "5b25191b-d830-490e-faf2-355b7a7c355a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/Notes/WOA7015 - Advanced Machine Learning/datasets/diabetes_012_health_indicators_BRFSS2015.csv'\n",
        "\n",
        "# csv_path = 'PATH NAME HERE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfPDoAGKrlW7"
      },
      "source": [
        "Downsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bWne3Vb1rk5r",
        "outputId": "9c96e96c-c5ec-4d64-e561-8ab88ac9bd71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, balanced_accuracy_score\n",
        "\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# extract last 15% of original dataset as test dataset\n",
        "test_start_index = len(df) // 100 * 85\n",
        "train_df = df.iloc[:test_start_index]\n",
        "test_df = df.iloc[test_start_index + 1:]\n",
        "\n",
        "min_class_count = train_df['Diabetes_012'].value_counts().min()\n",
        "class_groups = [train_df[train_df['Diabetes_012'] == c] for c in train_df['Diabetes_012'].unique()]\n",
        "downsampled_classes = [group.sample(n=min_class_count, random_state=42)\n",
        "                        for group in class_groups]\n",
        "\n",
        "train_df = pd.concat(downsampled_classes)\n",
        "\n",
        "train_features = train_df.drop(columns=[\"Diabetes_012\"])\n",
        "train_labels = train_df[\"Diabetes_012\"]\n",
        "\n",
        "test_features = test_df.drop(columns=[\"Diabetes_012\"])\n",
        "test_labels = test_df[\"Diabetes_012\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "count_class = train_labels.value_counts() # Count the occurrences of each class\n",
        "plt.bar(count_class.index, count_class.values)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I6bVLG0nsEK",
        "outputId": "c0603374-9ec2-4dfd-e9be-1857338eaa7b"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_features, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features.values, dtype=torch.float32).to(device) \n",
        "        self.targets = torch.tensor(targets.values, dtype=torch.long).to(device)    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "# Create Dataset\n",
        "train_dataset = DiabetesDataset(X_train, y_train)\n",
        "val_dataset = DiabetesDataset(X_val, y_val)\n",
        "test_dataset = DiabetesDataset(test_features, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCTcX2OoBOj"
      },
      "source": [
        "Model code starting here\n",
        "\n",
        "Use `train_dataset`, `val_dataset`, `test_dataset` as the input dataset\n",
        "\n",
        "Train Dataset has\n",
        "\n",
        "| Class | Count |\n",
        "| --- | --- |\n",
        "| 0.0 | 3978|\n",
        "| 2.0 | 3978|\n",
        "| 1.0 | 3978|\n",
        "\n",
        "Test Dataset has\n",
        "\n",
        "| Class | Count |\n",
        "| --- | --- |\n",
        "| 0.0 | 32427|\n",
        "| 2.0 | 5039|\n",
        "| 1.0 | 653|\n",
        "\n",
        "Example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7I4GlZjrnzA6"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLNN 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_kVsGjkxJOAU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, 32)\n",
        "        self.h1 = nn.Linear(32, 1024)\n",
        "        self.h2 = nn.Linear(1024, 64)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        x = F.relu(self.h1(x))\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = self.logsoftmax(self.output_layer(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j-aix9-IHS4",
        "outputId": "5442593f-2fbb-48ec-e675-36147bce64a8"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(X_train.columns)  # Number of input features\n",
        "num_classes = len(train_labels.unique())\n",
        "model = Net(input_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MultiMarginLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Inference on the test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted.tolist())\n",
        "    true_labels.extend(labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "z9A943AVxpLt",
        "outputId": "0810c62c-bb4b-4c11-b94b-2409ad5678bc"
      },
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
        "f1_score = f1_score(true_labels, predictions, average='weighted')\n",
        "print(f'Test accuracy: {accuracy:.6f}')\n",
        "print(f'Balanced Test accuracy: {balanced_accuracy:.6f}')\n",
        "print(f'F1 score: {f1_score:.6f}')\n",
        "print(f'{accuracy:.6f}')\n",
        "print(f'{balanced_accuracy:.6f}')\n",
        "print(f'{f1_score:.6f}')\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLNN 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, 128)\n",
        "        self.h1 = nn.Linear(128, 512)\n",
        "        self.h2 = nn.Linear(512, 1024)\n",
        "        self.h3 = nn.Linear(1024, 128)\n",
        "        self.output_layer = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.input_layer(x))\n",
        "        x = F.tanh(self.h1(x))\n",
        "        x = F.tanh(self.h2(x))\n",
        "        x = F.tanh(self.h3(x))\n",
        "        x = self.softmax(self.output_layer(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(X_train.columns)  # Number of input features\n",
        "num_classes = len(train_labels.unique())\n",
        "model = Net(input_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MultiMarginLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 300\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Inference on the test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted.tolist())\n",
        "    true_labels.extend(labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
        "f1_score = f1_score(true_labels, predictions, average='weighted')\n",
        "print(f'Test accuracy: {accuracy:.6f}')\n",
        "print(f'Balanced Test accuracy: {balanced_accuracy:.6f}')\n",
        "print(f'F1 score: {f1_score:.6f}')\n",
        "print(f'{accuracy:.6f}')\n",
        "print(f'{balanced_accuracy:.6f}')\n",
        "print(f'{f1_score:.6f}')\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLNN 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, 128)\n",
        "        self.bn0 = nn.BatchNorm1d(128)\n",
        "        self.h1 = nn.Linear(128, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.h2 = nn.Linear(1024, 5120)\n",
        "        self.bn2 = nn.BatchNorm1d(5120)\n",
        "        self.h3 = nn.Linear(5120, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.h4 = nn.Linear(1024, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.output_layer = nn.Linear(64, num_classes)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(self.input_layer(x))\n",
        "        x = self.bn0(x)\n",
        "        x = F.sigmoid(self.h1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.sigmoid(self.h2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.sigmoid(self.h3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = F.sigmoid(self.h4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.logsoftmax(self.output_layer(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(X_train.columns)  # Number of input features\n",
        "num_classes = len(train_labels.unique())\n",
        "model = Net(input_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MultiMarginLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Inference on the test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted.tolist())\n",
        "    true_labels.extend(labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
        "f1_score = f1_score(true_labels, predictions, average='weighted')\n",
        "print(f'Test accuracy: {accuracy:.6f}')\n",
        "print(f'Balanced Test accuracy: {balanced_accuracy:.6f}')\n",
        "print(f'F1 score: {f1_score:.6f}')\n",
        "print(f'{accuracy:.6f}')\n",
        "print(f'{balanced_accuracy:.6f}')\n",
        "print(f'{f1_score:.6f}')\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLNN 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, 128)\n",
        "        self.h1 = nn.Linear(128, 512)\n",
        "        self.h2 = nn.Linear(512, 1024)\n",
        "        self.h3 = nn.Linear(1024, 128)\n",
        "        self.output_layer = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.input_layer(x))\n",
        "        x = F.tanh(self.h1(x))\n",
        "        x = F.tanh(self.h2(x))\n",
        "        x = F.tanh(self.h3(x))\n",
        "        x = self.softmax(self.output_layer(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(X_train.columns)  # Number of input features\n",
        "num_classes = len(train_labels.unique())\n",
        "model = Net(input_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MultiMarginLoss(margin=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 300\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Inference on the test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predictions.extend(predicted.tolist())\n",
        "    true_labels.extend(labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
        "f1_score = f1_score(true_labels, predictions, average='weighted')\n",
        "print(f'Test accuracy: {accuracy:.6f}')\n",
        "print(f'Balanced Test accuracy: {balanced_accuracy:.6f}')\n",
        "print(f'F1 score: {f1_score:.6f}')\n",
        "print(f'{accuracy:.6f}')\n",
        "print(f'{balanced_accuracy:.6f}')\n",
        "print(f'{f1_score:.6f}')\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "19Zsozaex_25"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
